# 생성형 AI를 책임감 있게 사용하기

![생성형 AI를 책임감 있게 사용하기](../../images/03-lesson-banner.png?WT.mc_id=academic-105485-koreyst)

> **동영상 제공 예정**

인공지능, 특히 생성형 AI에 매료되기 쉽습니다. 그러나 책임감 있게 사용하는 방법에 대해서도 고려해야 합니다. 결과물이 공정하고 해롭지 않은지 확인하는 방법 등을 고려해야 하는데요. 이 장에서는 앞서 언급한 내용과 고려해야 할 사항, 그리고 AI 사용을 개선하기 위한 적극적인 조치를 취하는 방법을 소개합니다.

## 소개

이 강의에서 다룰 내용은 다음과 같습니다:

- 생성형 AI 어플리케이션을 구축할 때 책임감 있는 AI를 우선시해야 하는 이유.
- 책임감 있는 AI의 핵심 원칙과 이것이 생성형 AI와 어떻게 연관되는지.
- 전략과 도구를 통해 이러한 책임감 있는 AI 원칙을 실천하는 방법.

## 학습 목표

이 강의를 완료한 후 여러분은 다음을 알게 됩니다:

- 생성형 AI 어플리케이션을 구축할 때 책임감 있는 AI의 중요성.
- 책임감 있는 AI의 핵심 원칙을 생각하고 적용해야 하는 시점.
- 책임감 있는 AI의 개념을 실제로 적용하는 데 사용할 수 있는 도구와 전략.

## 책임감 있는 AI 원칙

생성형 AI에 대한 관심이 그 어느 때보다 높습니다. 이러한 열기는 이 분야에 많은 새로운 개발자, 관심, 투자를 불러일으켰습니다. 이는 생성형 AI를 사용해 제품과 회사를 만들려는 모든 이들에게 매우 긍정적인 일이지만, 책임감 있게 진행하는 것 또한 중요합니다.

이 과정에서 우리는 우리의 스타트업과 인공지능 교육 제품을 구축하는 데 중점을 둡니다. 우리는 책임 있는 AI의 원칙인 공정성, 포괄성, 신뢰성/안전성, 보안 및 개인 정보 보호, 투명성 및 책임을 사용할 것입니다. 이 원칙들을 통해 우리의 제품에서 창조적 인공지능을 사용하는 방법과 관련된 내용을 탐구할 것입니다.

## 책임감 있는 AI를 우선시해야 하는 이유

제품을 개발할 때 사용자의 최우선 관심사를 염두에 두고 인간 중심의 시각으로 접근하면 최상의 결과를 얻을 수 있습니다.

생성형 AI는 사용자에게 유용한 답변, 정보, 안내 및 콘텐츠를 생성할 수 있다는 점에서 독보적입니다. 이는 많은 수작업 단계 없이도 매우 인상적인 결과로 도출해냅니다. 하지만 적절한 계획과 전략이 없다면 안타깝게도 사용자, 제품, 사회 전체에 해로운 결과를 초래할 수도 있습니다.

이처럼 잠재적으로 해로운 결과를 초래할 수 있는 몇 가지 사례를 살펴보겠습니다:


### 환각(Hallucinations)

환각은 LLM이 완전히 말이 안 되거나 다른 정보 소스를 기반으로 사실적으로 틀린 것으로 알려진 내용을 생성할 때 사용되는 용어입니다.

예를 들어, 우리 스타트업에서 학생들이 모델에게 역사적 질문을 할 수 있는 기능을 구축한다고 가정해 봅시다. 한 학생이 `타이타닉호의 유일한 생존자는 누구였나요?`라는 질문을 합니다.

모델은 아래와 같은 응답을 생성합니다:

![Prompt saying "Who was the sole survivor of the Titanic"](../../images/ChatGPT-titanic-survivor-prompt.webp?WT.mc_id=academic-105485-koreyst)

> *(Source: [Flying bisons](https://flyingbisons.com?WT.mc_id=academic-105485-koreyst))*

이것은 매우 확신 있고 꼼꼼한 답변입니다. 안타깝지만 잘못된 답변입니다. 약간의 조사만 해도 타이타닉 호의 생존자가 한 명 이상이라는 것을 알 수 있습니다. 이 주제에 대해 막 연구하기 시작한 학생에게는 이 답변이 충분히 설득력 있어 의문을 제기하지 않고 사실로 받아들여질 수 있습니다. 그 결과 AI 시스템의 신뢰성이 떨어지고 스타트업의 평판에 부정적인 영향을 미칠 수 있습니다.

저희는 특정 LLM을 반복할 때마다 환각을 최소화하는 방향으로 성능이 개선되는 것을 확인했습니다.  이러한 개선에도 불구하고, 우리 어플리케이션 구축자와 사용자는 항상 이러한 제한 사항을 인지하고 있어야 합니다.

### 유해한 콘텐츠

이전 섹션에서 모델이 잘못되었거나 엉뚱한 응답을 생성하는 경우를 다루었습니다.  또 다른 위험은 모델이 유해한 콘텐츠로 응답하는 경우입니다.

유해한 콘텐츠는 다음과 같이 정의할 수 있습니다:

- 특정 집단에 자해 또는 위해를 가하도록 지시하거나 조장하는 내용.
- 혐오 또는 비하하는 콘텐츠.
- 모든 유형의 공격 또는 폭력 행위 계획을 안내하는 행위.
- 불법 콘텐츠를 찾거나 불법 행위를 저지르는 방법에 대한 지침을 제공하는 행위.
- 성적으로 노골적인 콘텐츠를 표시하는 행위.

스타트업인 저희는 이러한 유형의 콘텐츠를 학생들이 보지 못하도록 적절한 장치와 전략을 마련하고자 합니다.

### 공정성 부족

공정성이란 "AI 시스템이 편견과 차별이 없고 모든 사람을 공정하고 평등하게 대우하도록 보장하는 것"으로 정의됩니다. 생성형 AI의 생태계에서는 소외된 집단에 대한 차별적인 시각이 모델의 결과물에 의해 고착화되지 않기를 원합니다.

이러한 유형의 결과물은 사용자를 위한 긍정적인 제품 경험을 구축하는 데 방해가 될 뿐만 아니라 사회적으로도 해를 끼칠 수 있습니다. 애플리케이션 개발자는 생성형 AI로 솔루션을 구축할 때 항상 폭넓고 다양한 사용자층을 염두에 두어야 합니다.

## 생성형 AI를 책임감 있게 사용하는 방법

책임감 있는 생성형 AI의 중요성을 확인했으니, 이제 책임감 있게 AI 솔루션을 구축하기 위해 취할 수 있는 4가지 단계를 살펴보겠습니다:

![Mitigate Cycle](../../images/mitigate-cycle.png?WT.mc_id=academic-105485-koreyst)

### Measure Potential Harms

소프트웨어 테스트에서는 애플리케이션에 대한 사용자의 예정된 행동을 테스트합니다. 이처럼 사용자가 가장 많이 사용할 가능성이 높은 다양한 프롬프트 세트를 테스트하는 것은 잠재적 피해를 측정하는 좋은 방법입니다.

우리 스타트업은 교육용 제품을 만들고 있으므로 교육 관련 프롬프트 목록을 준비하는 것이 좋습니다. 특정 주제, 역사적 사실, 학생 생활에 대한 프롬프트 등을 다룰 수 있습니다.

### 잠재적 피해 완화하기


이제 이 모델과 그 답변에 따른 잠재적 피해를 예방하거나 한정시킬 수 있는 방법을 찾아야 할 때입니다. 이를 4가지 레이어로 살펴볼 수 있습니다:

![Mitigation Layers](../../images/mitigation-layers.png?WT.mc_id=academic-105485-koreyst)

- **모델(Model)**. 올바른 사용 사례에 알맞은 모델을 선택해야 합니다. GPT-4와 같이 규모가 크고 복잡한 모델은 더 작고 특정한 사용 사례에 적용될 때 해로운 콘텐츠의 위험이 더 클 수 있습니다. 교육 데이터를 사용하여 미세 조정하는 것도 해로운 콘텐츠의 위험을 줄입니다.

- **안전 시스템(Safety System)**. 안전 시스템은 모델을 제공하는 플랫폼에서 피해를 완화하기 위해 도움이 되는 도구 및 구성의 집합입니다. 예를 들어 Azure OpenAI 서비스의 콘텐츠 필터링 시스템을 들 수 있습니다. 시스템은 또한 탈옥 공격과 봇의 요청과 같은 의도치 않는 활동을 감지해야 합니다.

- **메타 프롬프트(Metaprompt)**. 메타프롬프트와 그라운딩은 특정 행동과 정보에 따라 모델을 지시하거나 제한하는 방법입니다. 이는 시스템 입력을 사용하여 모델의 특정 제한을 정의하는 것일 수 있습니다. 또한 시스템의 범위나 도메인에 더 관련된 출력을 제공하는 것을 포함합니다.

또한 검색 증강 생성(RAG)과 같은 기술을 사용하여 모델이 신뢰할 수 있는 일부 소스에서만 정보를 가져오도록 할 수도 있습니다. 이 코스 뒤에 [검색 어플리케이션 구축](../../../08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)에 대한 강의가 있습니다.

- **사용자 경험(User Experience)**. 마지막 레이어는 사용자가 애플리케이션의 인터페이스를 통해 어떤 방식으로든 모델과 직접 상호 작용하는 레이어입니다. 이러한 방식으로 사용자가 모델에 보낼 수 있는 입력 유형과 사용자에게 표시되는 텍스트 또는 이미지를 제한하도록 UI/UX를 설계할 수 있습니다. AI 애플리케이션을 배포할 때는 생성형 AI 애플리케이션이 할 수 있는 일과 할 수 없는 일에 대해서도 명확하게 알려야 합니다.  

전체 강의는 [AI 어플리케이션을 위한 UX 디자인](../../../12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)에서 확인할 수 있습니다.

- **모델 평가하기(Evaluate model)**. LLM(Large Language Models)과 함께 작업하는 것은 모델이 훈련된 데이터를 항상 제어할 수 없기 때문에 어려울 수 있습니다. 그럼에도 불구하고, 모델의 성능과 출력을 항상 평가해야 합니다. 모델의 정확성, 유사성, 근거성, 그리고 출력의 관련성을 측정하는 것은 여전히 중요합니다. 이는 이해관계자와 사용자에게 투명성과 신뢰를 제공하는 데 도움이 됩니다.

### 책임감 있는 생성형 AI 솔루션 운영하기

AI 어플리케이션을 중심으로 운영 사례를 구축하는 것이 마지막 단계입니다. 여기에는 법률 및 보안과 같은 스타트업의 다른 부서와 협력하여 모든 규제 정책을 준수하도록 보장하는 것이 포함됩니다. 또한 출시 전에 배포, 장애 처리, 롤백에 관한 계획을 수립하여 사용자 피해가 커지는 것을 방지하고자 합니다.

## Tools

책임감 있는 AI 솔루션을 개발하는 작업이 많아 보일 수 있지만, 그만한 가치가 있는 작업입니다. 생성형 AI의 영역이 커지면서 개발자들이 책임을 효율적으로 통합할 수 있도록 도와주는 도구들이 점점 더 발전할 것입니다. 예를 들어, [Azure AI Content Safety](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst )는 API 요청을 통해 유해한 콘텐츠 및 이미지를 탐지하는 데 도움이 될 수 있습니다.

## Knowledge check

책임 있는 AI 사용을 보장하기 위해 주의해야 할 사항은 무엇인가요?

1. 답변이 정확한지 확인합니다.
1. 유해한 용도, AI가 범죄 목적으로 사용되지 않아야 합니다.
1. AI에 편견과 차별이 없는지 확인합니다.

A: 2번과 3번이 정답입니다. 책임감 있는 AI는 유해한 영향과 편견을 완화하는 방법 등을 고려하는 데 도움이 됩니다.

## 🚀 도전해보기

[Azure AI 콘텐츠 세이프티](https://learn.microsoft.com/azure/ai-services/content-safety/overview?WT.mc_id=academic-105485-koreyst)를 읽고 사용 환경에 적용할 수 있는 사항을 알아보세요.

## 훌륭해요, 학습을 계속해보세요

이 강의를 완료한 후에는 [생성형 AI 학습 컬렉션](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)을 확인하여 생성형 AI 지식을 계속 업그레이드하세요!

4강에서는 [프롬프트 엔지니어링 기초](../../../04-prompt-engineering-fundamentals/translations/ko-kr/README.md?WT.mc_id=academic-105485-koreyst)를 살펴볼 예정입니다!